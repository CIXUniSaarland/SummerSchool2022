{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import scipy.stats as ss\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# set some styling defaults for matplotlib\n",
    "plt.style.use(\"seaborn-talk\")\n",
    "mpl.rcParams[\"figure.dpi\"] = 90  # change this to set apparent figure size\n",
    "mpl.rcParams[\"figure.figsize\"] = (7, 3)\n",
    "mpl.rcParams[\"figure.frameon\"] = False\n",
    "\n",
    "# set decimal precision to 3 dec. places\n",
    "%precision 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pymc3 \n",
    "# uncomment to install pymc3. \n",
    "# Note that this can be tricky on Windows; make sure you are in a conda environment or virtualenv!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc3\n",
    "import arviz as az\n",
    "\n",
    "# turn off extra messages\n",
    "import logging\n",
    "logger = logging.getLogger('pymc3')\n",
    "logger.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 3: Bayesian analysis\n",
    "## Outcomes\n",
    "\n",
    "* How to analyse experimental results from a Bayesian perspective\n",
    "    * How to build a simple model\n",
    "    * The effect of priors\n",
    "    * How to apply inference via MCMC\n",
    "    * How to interpret and report results   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal\n",
    "\n",
    "* analyse the result of experimental work using Bayesian principles. \n",
    "* be informed by priors\n",
    "* maintain uncertainty consistently\n",
    "* answer questions in terms of probabilities of hypotheses (*not* in terms of probabilities of data!).\n",
    "* report and visualise those results to answer questions we  might be interested in.\n",
    "\n",
    "\n",
    "## Task\n",
    "We are going to model the results of **Fitts' law** experiment, a classic HCI task. Fitts' law, very briefy, is a fairly consistent relationship observed in human pointing actions, like clicking an icon with a mouse. It says that there is an approximately linear relationship between the time taken to acquire a target and the target's \"index of difficulty\".\n",
    "\n",
    "The \"index of difficulty\" is just the logarithm of the ratio of the target size to its distance from the starting point -- it corresponds to the the log of the number of possible targets made by dividing up space into evenly spaced regions of fixed width.\n",
    "\n",
    "The basic formula looks like:\n",
    "\n",
    "$$MT = a + bID$$\n",
    "$$MT = a + b\\log_2\\left(\\frac{D}{W}\\right)$$\n",
    "\n",
    "$MT$ is the movement time (in seconds) and $ID$ is the index of difficulty (in bits).  $D$ and $W$ are distance and width, and $a$ and $b$ are parameters that *we don't know*. We'd expect $a$ and $b$ to be consistent for a particular pointing device/context, but to vary for different devices. For example, a high-DPI optical mouse might have a lower $b$ than a console controller joystick (i.e. can acquire more precise targets in the same time for the high-DPI mouse).\n",
    "\n",
    "### How does this relate to traditional frequentist statistics?\n",
    "\n",
    "<img src=\"linear_models.PNG\">\n",
    "\n",
    "[By Jonas Kristoffer Lindel√∏v: https://lindeloev.github.io/tests-as-linear/]\n",
    "\n",
    "## Process\n",
    "\n",
    "We are going to write down our data generating process, set priors on the parameters and then implement this in code using a probabilistic programming language. Once this is done, we will use MCMC sampling to draw samples from the prior and posterior distributions (and the prior and posterior *predictive* distributions). \n",
    "\n",
    "The power of a probabilistic programming language (PPL) means that the MCMC sampling step is *automatic*; we don't have to do anything to invert our model. We implement the DGP, and expose it to data.\n",
    "\n",
    "We can then summarise and compare the results of inference. We'll explore what effect setting priors has, and how we can use the resulting distributions to answer useful questions.\n",
    "\n",
    "## Package: `pymc3`\n",
    "We are going to use `pymc3` for inference. This is the most capable PPL for Python; there are other choices like Stan (own custom language; integrates well with R) with similar capabilities. \n",
    "\n",
    "`pymc3` defines a range of standard probability distributions associated with random variables, and allows us to write expressions over random variables. We can then directly introduced observed data to draw *samples* from the posterior distribution. `pymc3` works by MCMC sampling, so it only ever produces sequences of definite random samples, and it is subject to the approximation problems that MCMC approaches can have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, lets load some data. We assume this came from a human subjects trial [it didn't, I simulated it], where participants were asked to point at targets with variable widths and distances. Each participant repeated each acquisition several times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitts_data = pd.read_csv(\"fitts_a.csv\")\n",
    "fitts_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the data\n",
    "fitts_data.plot.scatter('ID', 'MT', s=5)\n",
    "plt.gca().set_xlabel(\"ID (bits)\")\n",
    "plt.gca().set_ylabel(\"Movement time (s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymc3_utils import pymc3_sample, trace_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a model\n",
    "\n",
    "Our data generating process is rather basic, but follows directly from the formula above. Note that we introduce a *noise term* to accommodate the imprecision of any experiment like this.\n",
    "\n",
    "So we have:\n",
    "$$MT = a + b ID + \\epsilon$$\n",
    "\n",
    "We'll assume, for simplicity, that $\\epsilon$ is normally distributed noise; this is a reasonable guess. So:\n",
    "\n",
    "$$MT = a + b ID + \\mathcal{N}(0, s)$$\n",
    "\n",
    "$s$ (the noise level) is also unknown, so we'll chuck it in the pile of parameters we want to infer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## generative model\n",
    "def generate_mt(a, b, std, id):\n",
    "    # returns seconds\n",
    "    return a + b * id + np.random.normal(0, std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_mt(0.1, 0.45, 0.1, 4.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need a log-likelihood; in this case, this is easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## generative model\n",
    "def lik_mt(a, b, std, id, mt):\n",
    "    # returns seconds\n",
    "    return ss.norm(a + b * id, std).logpdf(mt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lik_mt(0.1, 0.45, 0.1, 4.0, mt=1.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lik_mt(0.1, 0.45, 0.1, 4.0, mt=10.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pymc3 model\n",
    "We just need to translate this into the pymc3 primitives, and set priors for a, b, s. We'll make some very weak guesses about what a,b,s might be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pymc3.Model() as fitts_model:\n",
    "    # define the model\n",
    "    a = pymc3.Normal(\"a\", 0, 1)     # N(0,1)           \n",
    "    b = pymc3.Normal(\"b\", 1)        # N(0,1)\n",
    "    std_prior = pymc3.HalfNormal(\"sigma\", 1)        # HN(0,1)\n",
    "    # note: the prior here has no effect on inference\n",
    "    # but it is useful for simulating without data\n",
    "    ids = pymc3.Uniform(\"ids\", 0.0, 5.0, observed=fitts_data[\"ID\"])        # U(0,5)\n",
    "    lin = a + b * ids\n",
    "    # mt = N(a+b*id, sigma)\n",
    "    mt = pymc3.Normal(\"mt\", mu=lin, sigma=std_prior, observed=fitts_data[\"MT\"])\n",
    "    trace = pymc3_sample()\n",
    "    trace[\"observed\"] = {\"mt\":fitts_data[\"MT\"], \"id\":fitts_data[\"ID\"]}\n",
    "    \n",
    "fitts_result = trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summarising results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace[\"posterior\"][\"a\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(az.hdi(trace[\"posterior\"][\"a\"]))\n",
    "print(az.hdi(trace[\"posterior\"][\"b\"]))\n",
    "print(az.hdi(trace[\"posterior\"][\"sigma\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "az.plot_trace(trace[\"posterior\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(trace[\"posterior\"][\"a\"]), np.mean(trace[\"posterior\"][\"b\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "trace_hist(ax, trace[\"posterior\"]['b'], \"b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prior vs posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare prior and posterior\n",
    "fig, ax = plt.subplots(ncols=3)\n",
    "def prior_posterior(ax, var):\n",
    "    ax.boxplot([trace[\"prior\"][var], trace[\"posterior\"][var]], labels=[\"Prior\", \"Posterior\"])\n",
    "    ax.set_title(var)\n",
    "    \n",
    "prior_posterior(ax[0], \"a\")\n",
    "prior_posterior(ax[1], \"b\")\n",
    "prior_posterior(ax[2], \"sigma\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_model(all_trace, mode=\"posterior\"):\n",
    "    trace = all_trace[mode]\n",
    "    fig = plt.figure(constrained_layout=True, figsize=(9,4))\n",
    "    gs = fig.add_gridspec(3, 3)\n",
    "    a_plot = fig.add_subplot(gs[0,0])\n",
    "    b_plot = fig.add_subplot(gs[0,1])\n",
    "    sig_plot = fig.add_subplot(gs[0,2])\n",
    "    pred_plot = fig.add_subplot(gs[1:3,:])\n",
    "    \n",
    "    trace_hist(a_plot, trace[\"a\"], \"a\")\n",
    "    trace_hist(b_plot, trace[\"b\"], \"b\")\n",
    "    trace_hist(sig_plot, trace[\"sigma\"], \"$\\sigma$\")\n",
    "    a_plot.set_xlim(-1, 1)\n",
    "    b_plot.set_xlim(-1, 1)\n",
    "    sig_plot.set_xlim(0, 1)\n",
    "    x_range = np.linspace(0.5, 5, 20)\n",
    "    pred_plot.set_xlim(0, 5)\n",
    "    pred_plot.set_ylim(0, 3)\n",
    "    pred_plot.set_xlabel(\"Index of difficulty\")\n",
    "    pred_plot.set_ylabel(\"Movement time\")\n",
    "    samples = 500\n",
    "    for _a, _b in zip(trace[\"a\"][:samples], trace[\"b\"][:samples]):        \n",
    "        pred_plot.plot(x_range, _a + _b * x_range, alpha=0.1, c='C2', lw=0.75)\n",
    "    if mode==\"posterior\":\n",
    "        pred_plot.scatter(all_trace[\"observed\"][\"id\"], all_trace[\"observed\"][\"mt\"], c='k', zorder=10, s=2)\n",
    "        n = len(all_trace[\"observed\"][\"id\"])\n",
    "        fig.suptitle(f\"N={n}\")\n",
    "    else:\n",
    "        fig.suptitle(\"Prior\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_model(trace, mode='posterior')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prior checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_model(trace, mode='prior')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimenting\n",
    "Let's play around with the model; I'll wrap the model in a function to make it easy to tweak priors and subsample data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sample_model(data_id, data_mt, a_prior=1, b_prior=2, std_prior=1, a_mean_prior=0, b_mean_prior=0, b_positive=False, ns=None):        \n",
    "    # choose the a subset if requested\n",
    "    if ns:\n",
    "        subset = np.random.randint(0, data_id.shape[0], ns)\n",
    "        data_id = data_id[subset]\n",
    "        data_mt = data_mt[subset]\n",
    "    \n",
    "    with pymc3.Model() as model:        \n",
    "        a = pymc3.Normal(\"a\", a_mean_prior, a_prior)        \n",
    "        if b_positive:\n",
    "            b = pymc3.HalfNormal(\"b\", b_prior)    \n",
    "        else:\n",
    "            b = pymc3.Normal(\"b\", b_mean_prior, b_prior)                \n",
    "        std_prior = pymc3.HalfNormal(\"sigma\", std_prior)        \n",
    "        ids = pymc3.Uniform(\"ids\", 0.0, 5.0, observed=data_id)        \n",
    "        lin = a + b * ids        \n",
    "        mt = pymc3.Normal(\"mt\", mu=lin, sigma=std_prior, observed=data_mt)\n",
    "        return pymc3_sample(observed={\"mt\":data_mt, \"id\":data_id})        \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What if we assumed different priors?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Weaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_weak = sample_model(fitts_data[\"ID\"], fitts_data[\"MT\"], a_prior=5, b_prior=5, std_prior=5);\n",
    "show_model(result_weak)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stronger (more informed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_strong = sample_model(fitts_data[\"ID\"], fitts_data[\"MT\"], a_prior=0.1, b_prior=0.1, \n",
    "                             a_mean_prior=0.1, b_mean_prior=0.4, std_prior=0.3);\n",
    "show_model(result_strong, mode='prior')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_model(result_strong)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stronger (badly informed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_bad = sample_model(fitts_data[\"ID\"], fitts_data[\"MT\"], a_prior=0.1, b_prior=0.1, a_mean_prior=1.0, b_mean_prior=-0.4, std_prior=0.05);\n",
    "show_model(result_bad, mode='prior')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_model(result_bad, mode='posterior')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What happens if we had less data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sub in [5, 20, 100]:\n",
    "    result_sub = sample_model(fitts_data[\"ID\"], fitts_data[\"MT\"], ns=sub);\n",
    "    show_model(result_sub, mode='posterior')\n",
    "    plt.suptitle(f\"Subsample = {sub}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Different prior, with small data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a strong prior, but small sample size\n",
    "result_strong_small = sample_model(fitts_data[\"ID\"], fitts_data[\"MT\"], a_prior=0.1, b_prior=0.1, a_mean_prior=0.1, b_mean_prior=0.4, std_prior=0.3, ns=5);\n",
    "show_model(result_strong_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a weak prior, but small sample size\n",
    "result_weak_small = sample_model(fitts_data[\"ID\"], fitts_data[\"MT\"], a_prior=5, b_prior=5, std_prior=5, ns=5);\n",
    "show_model(result_weak_small)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A smaller dataset with a different device\n",
    "Let's imagine we also get some data from a different device. Some changes:\n",
    "\n",
    "* We have only three participants, and they only tried each target once\n",
    "* The target widths and distances were chosen randomly, not systematically\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitts_alt = pd.read_csv(\"fitts_b.csv\")\n",
    "fitts_alt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the data\n",
    "fitts_alt.plot.scatter('ID', 'MT', s=5)\n",
    "plt.gca().set_xlabel(\"ID (bits)\")\n",
    "plt.gca().set_ylabel(\"Movement time (s)\")\n",
    "plt.title(\"Fitts data (alternative device)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_result = sample_model(fitts_alt[\"ID\"], fitts_alt[\"MT\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_model(alt_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying\n",
    "How might we use or report these results? One thing we might want to do is to decide if the \"alternate\" device is superior or inferior to the original device.\n",
    "\n",
    "### Does Fitts' law fit?\n",
    "This question is answered by how well we can identify $a$ and $b$ from the data. We can see pretty clearly that the parameters are identified fairly precisely, and that the residual noise ($\\sigma$) is small. There are other ways of quantifying this (especially if comparing multiple models), but a simple visual inspection is suitable here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_forest(fitts_result[\"posterior\"], combined=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(fitts_result[\"posterior_predictive\"][\"ids\"].ravel(), fitts_result[\"posterior_predictive\"][\"mt\"].ravel(), s=1, alpha=0.01)\n",
    "ax.set_title(\"Posterior predictive\")\n",
    "ax.set_xlabel(\"ID\")\n",
    "ax.set_ylabel(\"MT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posterior comparisons\n",
    "We have lots of ways we could look at this. If we think a smaller $b$ means better, we could compare distributions of $b$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "trace_hist(ax, fitts_result[\"posterior\"][\"b\"], 'b', c='C2')\n",
    "trace_hist(ax, alt_result[\"posterior\"][\"b\"], 'b', c='C3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "differences = fitts_result[\"posterior\"][\"b\"] - alt_result[\"posterior\"][\"b\"]\n",
    "print(np.mean(differences)) # average increase/decrease\n",
    "print(np.mean(differences<0)) # probability of superiority"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see there is very little overlap here; these have very different credible intervals. Or we could compare how noisy we think the experiments were:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "trace_hist(ax, fitts_result[\"posterior\"][\"sigma\"], 'sigma', c='C2')\n",
    "trace_hist(ax, alt_result[\"posterior\"][\"sigma\"], 'sigma', c='C3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the generative model to make predictions\n",
    "We can also make concrete predictions, since our model is generative. For example, what is the expected increase in time to select a target of ID=5?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(a, b, std, id):\n",
    "    return a + b * id + np.random.normal(0, 1, std.shape)*std\n",
    "\n",
    "MT_std =  model(fitts_result[\"posterior\"][\"a\"], fitts_result[\"posterior\"][\"b\"], fitts_result[\"posterior\"][\"sigma\"], id=5.0)\n",
    "MT_alt =  model(alt_result[\"posterior\"][\"a\"], alt_result[\"posterior\"][\"b\"], alt_result[\"posterior\"][\"sigma\"], id=5.0)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "trace_hist(ax, MT_std, 'Movement time', c='C2')\n",
    "trace_hist(ax, MT_alt, 'Movement time', c='C3')\n",
    "\n",
    "print(np.mean(MT_alt - MT_std))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflection\n",
    "\n",
    "### What was gained?\n",
    "* We can directly incorporate priors that make sense (e.g. informed by previous experiments).\n",
    "* We get full distributions in our results, not summary statistics.\n",
    "    * We can then choose the measure we want to use to report the results\n",
    "* We don't have to worry about choosing the right test, or how to interpret p-values\n",
    "    * in fact, we don't have to make dichotomous decisions at all\n",
    "* We can answer questions we're actually interested in, like \"how much longer will pointing at a 5 pixel target on the edge of the screen take with a console joystick\"?\n",
    "### What were the challenges?\n",
    "* We had to be able to write our problem down in pymc3; this required a bit of finesse.\n",
    "* Inference wasn't instant, even for this relatively simple problem. It just gets slower from here!\n",
    "* Reporting the results requires choices -- it's not the case that we can just to a standardised process for every problem. \n",
    "* Setting priors required some thought; though as we saw, it often makes little difference to our inferences.\n",
    "### What else could we have done\n",
    "* Our model is very simple. We could easily have incorporated more details; perhaps further polynomial terms, like $MT = a + bID + cID^2$.\n",
    "* More usefully, we could have made a more generative model -- one that predicts cursor *trajectories*, not just times, and fitted that to the data in exactly the same way.\n",
    "* Or we could model different participants having personal a,b values, but where a,b values are drawn from a common population distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by participants\n",
    "participant_ids = np.array([data[\"ID\"] for group, data in fitts_data.groupby(\"participant\")])\n",
    "participant_mts = np.array([data[\"MT\"] for group, data in fitts_data.groupby(\"participant\")])\n",
    "n_participants = len(participant_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"partially pooled Bayesian hierarchical model\"\n",
    "\n",
    "with pymc3.Model() as model:\n",
    "    # population a, b\n",
    "    a_pop = pymc3.Normal(\"a_pop\", 0.1, 1)\n",
    "    b_pop = pymc3.Normal(\"b_pop\", 0.5, 1)\n",
    "    \n",
    "    # common std. dev.\n",
    "    a_std = pymc3.HalfNormal(\"a_std\", 0.2)\n",
    "    b_std = pymc3.HalfNormal(\"b_std\", 0.2)\n",
    "    # common noise\n",
    "    std_prior = pymc3.HalfNormal(\"sigma\", 1)        # HN(0,1)\n",
    "    \n",
    "    ids = pymc3.Uniform(\"ids\", 0.0, 5.0, observed=participant_ids.T)        # U(0,5)\n",
    "    # individual a,b\n",
    "    a = pymc3.Normal(\"a\", a_pop, a_std, shape=n_participants)\n",
    "    b = pymc3.Normal(\"b\", b_pop, b_std, shape=n_participants)\n",
    "    lin = a + b * ids\n",
    "    mt = pymc3.Normal(\"mt\", mu=lin, sigma=std_prior, observed=participant_mts.T)\n",
    "    grouped_result = pymc3_sample()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_forest(grouped_result[\"posterior\"][\"b\"].T)\n",
    "plt.xlabel(\"b\")\n",
    "plt.ylabel(\"Participant ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# az.from_pymc3(grouped_result[\"posterior\"]).to_dataframe(filter_groups=\"like\", groups=\"posterior\").to_csv(\"fitts_pooled_model.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
