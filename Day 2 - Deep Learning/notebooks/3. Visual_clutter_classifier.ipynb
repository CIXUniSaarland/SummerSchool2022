{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u06VXbls6pxu"
   },
   "source": [
    "![ACM SIGCHI Summer School on Computational Interaction  \n",
    "Inference, optimization and modeling for the engineering of interactive systems  \n",
    "13th June - 18th June 2022  \n",
    "Saarland University in Saarbrücken, Germany](imgs/header.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual clutter classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aIHorF-c7FDZ"
   },
   "source": [
    "## 🔑  Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jSm2bLrs7Jxr"
   },
   "source": [
    "### Layout composition \n",
    "A website layout is a pattern that defines a website’s structure. It has the role of structuring the information present on a site both for the website’s owner and for users. It provides clear paths for navigation within webpages and puts the most important elements of a website front and center.\n",
    "\n",
    "There are many reasons to consider a choice of layout wisely, for example:\n",
    "- A good layout keeps users on the site because it makes important information easily accessible and intuitive to find. A bad layout frustrates users which then quickly leaves the site because they can’t find what they are looking for.\n",
    "\n",
    "- There’s a strong [relationship](https://blog.hubspot.com/marketing/compelling-stats-website-design-optimization-list) between the layout and the engagement of users with the website. It determines how long they dwell on the website pages, how many pages they browse and how often they come back to the website.\n",
    "\n",
    "- According to Adobe research, 38% of people will stop engaging with a website if the content/layout is unattractive.\n",
    "\n",
    "You can think about other reasons why layouts are important. Once you have no doubts, you can start the following exercise.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Clutter is an important factor in UI\n",
    "design and information visualization. In this exercise you\n",
    "will develop a DL model that can predict automatically how visually\n",
    "packed and/or disorganized a UI is. Overall, cluttered UIs cause\n",
    "decreased recognition performance, therefore this model has potential\n",
    "for providing UI designers with \"a priori\" estimations of visual\n",
    "clutter, without having to actually look at the UI. This model could be\n",
    "offered as a third-party service or integrated into an existing one\n",
    "such as https://interfacemetrics.aalto.fi/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KNza7yvgSxSX"
   },
   "outputs": [],
   "source": [
    "%tensorflow_version 1.x # this tutorial use TensorFlow v.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xtzd8349GEWi"
   },
   "source": [
    "## 1. Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "VlCrbEFx2ph2",
    "outputId": "baa0d48a-1f8d-4408-afe4-5d724234e6a1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from tensorflow.python.keras.models import Sequential #The Sequential model is a linear stack of layers\n",
    "from tensorflow.python.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.python.keras.layers import Input, Conv2D, MaxPooling2D, Activation, Dropout, Flatten, Dense\n",
    "from tensorflow.python.keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping\n",
    "\n",
    "# Some GPUs require setting the `allow_growth` setting.\n",
    "# Comment out this code is you don't have a GPU card.\n",
    "import tensorflow.compat.v1 as tf\n",
    "#config = tf.ConfigProto()\n",
    "#config.gpu_options.allow_growth = True\n",
    "#tf.Session(config=config)\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AwGic1BFhJZh"
   },
   "outputs": [],
   "source": [
    "!unzip data/layouts-easy.zip #extract data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WlsjnUhn-qGY"
   },
   "source": [
    "## 2. Load data\n",
    "\n",
    "### Dataset description\n",
    "The data for today's exericise session is a number of layouts which can be categorized into two groups:\n",
    "- <font color=green>good</font> cop (layout)\n",
    "- <font color=red>bad</font> cop (layout)\n",
    "\n",
    "The distinction is based on the composition of the layout.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UNlb9JEWBiAl"
   },
   "source": [
    "#### **Bad layout 😭**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 381
    },
    "colab_type": "code",
    "id": "Y05dyKxF_W8I",
    "outputId": "aa0fcdca-a0ff-49c3-e350-3a1e08deddb0"
   },
   "source": [
    "![bad](https://media.giphy.com/media/IhgEVrZZ2p8g5g3pFh/giphy.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QqezLkVsBoK8"
   },
   "source": [
    "#### **Good Layout😀**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 381
    },
    "colab_type": "code",
    "id": "lchEZ4ivB02f",
    "outputId": "1e66828f-5309-4465-cc22-fcf77ce19eb7"
   },
   "source": [
    "![good](https://media.giphy.com/media/lSE2LCTTrae4GEy4Km/giphy.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AppRHHNCHl6B"
   },
   "source": [
    "The following code assigns the hyperparameters such as the number of epochs and the batch size. The choice of these hyperparameters affects the accuracy and the training time of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "e0_FV4Ls2vnH",
    "outputId": "d5368c13-76e0-494d-e411-221659e910d5"
   },
   "outputs": [],
   "source": [
    "IMAGE_SIZE = (200, 200)\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class labels: ['Bad', 'Good']\n"
     ]
    }
   ],
   "source": [
    "dataset_dir = 'layouts-easy'\n",
    "trainer_dir = dataset_dir + '/train'\n",
    "\n",
    "# Specify manually the class labels, otherwise Keras will assign alphanumeric values\n",
    "# as directories are listed (in no particular order).\n",
    "#os.walk() yields a 3-tuple (dirpath, dirnames, filenames).\n",
    "dirname, class_labels, _= next(os.walk(trainer_dir))\n",
    "print('Class labels: {}'.format(class_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Ek__v4kIdk0"
   },
   "source": [
    "###### Data augmentation\n",
    "\n",
    "\n",
    "Data augmentation is a strategy that enables practitioners to significantly increase the diversity of data available for training models, without actually collecting new data.\n",
    "\n",
    "Basic augmentation techniques are commonly used to train large neural networks:\n",
    "- cropping\n",
    "- padding\n",
    "- horizontal flipping\n",
    "\n",
    "More info on data augmentation can be found [here](https://nanonets.com/blog/data-augmentation-how-to-use-deep-learning-when-you-have-limited-data-part-2/).\n",
    "\n",
    "> Keras ```ImageDataGenerator``` is a gem! It lets you augment your images in real-time while your model is still training! You can apply any random transformations on each training image as it is passed to the model. This will not only make your model robust but will also save up on the overhead memory!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A5hlRVRy2-B0"
   },
   "outputs": [],
   "source": [
    "# Apply some data augmentation techniques.\n",
    "image_data = ImageDataGenerator(\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e0txis1gJOCs"
   },
   "source": [
    "*Python Note*🤓\n",
    "\n",
    "In the next cell we use <font color=green>*generators*</font> - simple functions which return an iterable set of items, one at a time, in a special way.\n",
    "\n",
    "When an iteration over a set of item starts using the for statement, the generator is run. Once the generator's function code reaches a \"yield\" statement, the generator yields its execution back to the for loop, returning a new value from the set. The generator function can generate as many values (possibly infinite) as it wants, yielding each one in its turn.\n",
    "\n",
    "You can write the following code to understand better the logic generator call uses:\n",
    "\n",
    "\n",
    "```\n",
    "import random\n",
    "\n",
    "def lottery():\n",
    "    # returns 6 numbers between 1 and 40\n",
    "    for i in range(6):\n",
    "        yield random.randint(1, 40)\n",
    "\n",
    "    # returns a 7th number between 1 and 15\n",
    "    yield random.randint(1,15)\n",
    "\n",
    "for random_number in lottery():\n",
    "       print(\"And the next number is... %d!\" %(random_number))\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "AeboN4B93BTi",
    "outputId": "fb0e1d63-39ce-46b2-ebab-2c13d3884e0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 320 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Use generators since we might not be able to fit all the images into memory.\n",
    "train_generator = image_data.flow_from_directory(\n",
    "    trainer_dir,\n",
    "    subset='training',\n",
    "    color_mode='grayscale',\n",
    "    target_size=IMAGE_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical', \n",
    "    classes=class_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "wX-ZGnja3CQT",
    "outputId": "da20fb6d-cf92-4266-8087-c1385a6e517a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 80 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "valid_generator = image_data.flow_from_directory(\n",
    "    trainer_dir,\n",
    "    subset='validation',\n",
    "    color_mode='grayscale',\n",
    "    target_size=IMAGE_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    classes=class_labels\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9CqxvShELgk3"
   },
   "source": [
    "## 3. Model training\n",
    "\n",
    "As you will see we use <font color=orange>ReLU</font> activation function in our model. ReLU stands for *rectified linear unit* Mathematically, it is defined as $f(x_i) = max(0, x_i)$. \n",
    "\n",
    "Visually, it looks like the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "n5AcJ1i1MA7s",
    "outputId": "4eb9c3c4-8801-4764-cba6-975911d2af13"
   },
   "source": [
    "![relu](https://miro.medium.com/max/1026/1*DfMRHwxY1gyyDmrIAd-gjQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s7FozNu7MFns"
   },
   "source": [
    "ReLU is the most commonly used activation function in neural networks, especially in [CNNs](https://www.youtube.com/watch?v=YRhxdVk_sIs). \n",
    "\n",
    "If you are unsure what activation function to use in your network, ReLU is usually a good first choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 457
    },
    "colab_type": "code",
    "id": "_HCh3qge3EBI",
    "outputId": "3ee0fdd2-9d42-461c-8ba4-b821f0092c7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 198, 198, 3)       30        \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 99, 99, 3)         0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 99, 99, 3)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 29403)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 58808     \n",
      "=================================================================\n",
      "Total params: 58,838\n",
      "Trainable params: 58,838\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Notice that images are grayscaled, therefore we have to set the number of channels to 1.\n",
    "# Image shape is `(width, height, num_channels)` in TensorFlow.\n",
    "in_shape = IMAGE_SIZE + (1,)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(3, (3,3), activation='relu', input_shape=in_shape))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WmplUgdo3HKn"
   },
   "outputs": [],
   "source": [
    "# Setup some useful callbacks.\n",
    "#visualizer = TensorBoard(log_dir='/tmp/layouts')\n",
    "#checkpoint = ModelCheckpoint('{}-best.h5'.format(dataset_dir), monitor='val_acc', mode='max', save_best_only=True)\n",
    "earlystops = EarlyStopping(patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "RuIZMIO03JFL",
    "outputId": "fa7f7dbf-708c-461a-b869-56aa851ec455"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 10s 2s/step - loss: 12.2280 - accuracy: 0.5250 - val_loss: 10.7759 - val_accuracy: 0.6250\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 8s 2s/step - loss: 8.8444 - accuracy: 0.7031 - val_loss: 12.9213 - val_accuracy: 0.6562\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 8s 2s/step - loss: 9.1573 - accuracy: 0.6938 - val_loss: 4.7931 - val_accuracy: 0.8281\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 8s 2s/step - loss: 6.9396 - accuracy: 0.7844 - val_loss: 7.6810 - val_accuracy: 0.7344\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 8s 2s/step - loss: 7.9143 - accuracy: 0.7563 - val_loss: 14.8970 - val_accuracy: 0.6250\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 8s 2s/step - loss: 9.4503 - accuracy: 0.7719 - val_loss: 6.9813 - val_accuracy: 0.7656\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 8s 2s/step - loss: 5.8745 - accuracy: 0.7906 - val_loss: 8.7825 - val_accuracy: 0.7500\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 8s 2s/step - loss: 8.0416 - accuracy: 0.7563 - val_loss: 6.8746 - val_accuracy: 0.7656\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 8s 2s/step - loss: 4.5472 - accuracy: 0.8594 - val_loss: 7.5958 - val_accuracy: 0.7500\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 8s 2s/step - loss: 4.7308 - accuracy: 0.8500 - val_loss: 5.4119 - val_accuracy: 0.7812\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 8s 2s/step - loss: 5.2089 - accuracy: 0.8281 - val_loss: 10.5187 - val_accuracy: 0.7500\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 8s 2s/step - loss: 4.9236 - accuracy: 0.8313 - val_loss: 3.1190 - val_accuracy: 0.8438\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 8s 2s/step - loss: 4.8093 - accuracy: 0.8562 - val_loss: 7.6373 - val_accuracy: 0.8438\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 8s 2s/step - loss: 7.3936 - accuracy: 0.7906 - val_loss: 3.1234 - val_accuracy: 0.8281\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 8s 2s/step - loss: 8.6290 - accuracy: 0.7812 - val_loss: 5.0709 - val_accuracy: 0.8281\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 8s 2s/step - loss: 4.5527 - accuracy: 0.8500 - val_loss: 7.1147 - val_accuracy: 0.7500\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 8s 2s/step - loss: 4.4454 - accuracy: 0.8438 - val_loss: 7.1364 - val_accuracy: 0.7969\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 8s 2s/step - loss: 6.0117 - accuracy: 0.8156 - val_loss: 8.6723 - val_accuracy: 0.7344\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 8s 2s/step - loss: 7.7843 - accuracy: 0.8188 - val_loss: 10.5423 - val_accuracy: 0.7188\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 8s 2s/step - loss: 4.8630 - accuracy: 0.8562 - val_loss: 5.9249 - val_accuracy: 0.8750\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 8s 2s/step - loss: 6.0256 - accuracy: 0.8625 - val_loss: 3.4300 - val_accuracy: 0.8906\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 8s 2s/step - loss: 3.3962 - accuracy: 0.8969 - val_loss: 6.6712 - val_accuracy: 0.7812\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x16a57f1f0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model.\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    epochs=NUM_EPOCHS,\n",
    "    steps_per_epoch=train_generator.samples // BATCH_SIZE,\n",
    "    validation_data=valid_generator,\n",
    "    validation_steps=valid_generator.samples // BATCH_SIZE,\n",
    "    callbacks=[earlystops]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "qDJcaUx23LCQ",
    "outputId": "c1c10793-fa73-4e30-84a3-b06105566d7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 208ms/step - loss: 2.9090 - accuracy: 0.9000\n",
      "Accuracy: 90.00%\n"
     ]
    }
   ],
   "source": [
    "# Report score from last epoch.\n",
    "loss, acc = model.evaluate(valid_generator)\n",
    "print('Accuracy: {:.2f}%'.format(acc*100))\n",
    "\n",
    "# Finally save the model.\n",
    "model.save('{}.h5'.format(dataset_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "REp-E996NPrh"
   },
   "source": [
    "As you can see our model showed a result reaching the accuracy of 90%. \n",
    "\n",
    "Now, we use our model to make predictions on the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0dAjiwmdOkSi"
   },
   "source": [
    "## 4. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mMUmunHR3V-v"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.python.keras.models import load_model\n",
    "from tensorflow.python.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "UUOWl66S3uH2",
    "outputId": "f872b229-965f-499a-8411-83954db4d5f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class labels: ['Bad', 'Good']\n"
     ]
    }
   ],
   "source": [
    "dataset_dir = 'layouts-easy'\n",
    "trainer_dir = dataset_dir + '/train'\n",
    "\n",
    "# Specify manually the class labels, otherwise Keras will assign alphanumeric values\n",
    "# as directories are listed (in no particular order).\n",
    "dirname, class_labels, _ = next(os.walk(trainer_dir))\n",
    "print('Class labels: {}'.format(class_labels))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DRIRYY553yON"
   },
   "outputs": [],
   "source": [
    "# Don't compile the model when testing new data.\n",
    "model = load_model('{}.h5'.format(dataset_dir), compile=False) #compile the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Don't shuffle the data, so that we can get the groundtruth labels later.\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "        dataset_dir + '/test',\n",
    "        target_size=(200, 200),\n",
    "        color_mode='grayscale',\n",
    "        shuffle = False,\n",
    "        class_mode='categorical',\n",
    "        classes=class_labels,\n",
    "        batch_size=1)\n",
    "\n",
    "filenames = test_generator.filenames\n",
    "nb_samples = len(filenames)\n",
    "predict = model.predict(test_generator,steps = nb_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v0yA5kR832v_"
   },
   "outputs": [],
   "source": [
    "#Let's use the model to predict new data and then compare the predictions against the test labels.\n",
    "\n",
    "y_true = list(map(lambda f : os.path.dirname(f), test_generator.filenames)) #ground turth\n",
    "predicted_class_indices=np.argmax(predict,axis=1)\n",
    "y_pred = list(map(lambda p : class_labels[0] if p == 0 else class_labels[1], predicted_class_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "MxzEMrMQ346L",
    "outputId": "d359aa13-c8f5-4700-8340-f25e4b1c3fa6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 82.05%\n",
      "Recall: 82.00%\n",
      "F-measure: 81.99%\n"
     ]
    }
   ],
   "source": [
    "# Let's see how good those predictions were.\n",
    "precision, recall, fmeasure, _ = precision_recall_fscore_support(y_true, y_pred, labels=class_labels, average='weighted')\n",
    "print('Precision: {:.2f}%'.format(precision * 100))\n",
    "print('Recall: {:.2f}%'.format(recall * 100))\n",
    "print('F-measure: {:.2f}%'.format(fmeasure * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b5eD7PnX6Z9P"
   },
   "source": [
    "## 🏁 6. Conclusion\n",
    "\n",
    "Now, you know:\n",
    "\n",
    "1.   how important is layout composition\n",
    "2.   what is data augmentation\n",
    "3.   what is a generator function in Python\n",
    "4.   how to apply a simple CNN model\n",
    "5.   what is a ReLU activation function\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Layout_exercise.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
