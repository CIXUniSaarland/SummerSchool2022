{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f0c9ff2",
   "metadata": {},
   "source": [
    "![ACM SIGCHI Summer School on Computational Interaction  \n",
    "Inference, optimization and modeling for the engineering of interactive systems  \n",
    "13th June - 18th June 2022  \n",
    "Saarland University in Saarbr√ºcken, Germany](imgs/header.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b122c503",
   "metadata": {},
   "source": [
    "# <font face=\"gotham\" color=\"Brown\">  Getting started with  </font>\n",
    "\n",
    "\n",
    "![PyTorch](https://keras.io/img/logo.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cf95b1",
   "metadata": {},
   "source": [
    "Official resources:\n",
    "* [Getting started ](https://keras.io/getting_started/)\n",
    "* [Introduction to Keras for engineers](https://keras.io/getting_started/intro_to_keras_for_engineers/)\n",
    "* [Introduction to Keras for researchers.](https://keras.io/getting_started/intro_to_keras_for_researchers/)\n",
    "\n",
    "\n",
    "Keras is an open-source software library that provides a Python interface for artificial neural networks. Keras acts as an interface for the TensorFlow library.\n",
    "![keras](imgs/tf.keras.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522c9631",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Basic concepts\n",
    "\n",
    "Tensors\n",
    "--------------------------------------------\n",
    "\n",
    "Tensors are a specialized data structure that are very similar to arrays\n",
    "and matrices. We use tensors to encode the inputs and\n",
    "outputs of a model, as well as the model‚Äôs parameters.\n",
    "\n",
    "Tensors are similar to NumPy‚Äôs ndarrays, except that tensors can run on\n",
    "GPUs or other specialized hardware to accelerate computing. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24123ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ccf11f",
   "metadata": {},
   "source": [
    "Let's take a look at the object that is at the core of TensorFlow: the Tensor.\n",
    "\n",
    "Here's a constant tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "186ab560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[5 2]\n",
      " [1 3]], shape=(2, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant([[5, 2], [1, 3]])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4cf99f",
   "metadata": {},
   "source": [
    "You can get its value as a NumPy array by calling .numpy():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53a23043",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5, 2],\n",
       "       [1, 3]], dtype=int32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4a84ba",
   "metadata": {},
   "source": [
    "Much like a NumPy array, it features the attributes dtype and shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77ca2122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dtype: <dtype: 'int32'>\n",
      "shape: (2, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"dtype:\", x.dtype)\n",
    "print(\"shape:\", x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfeba8c",
   "metadata": {},
   "source": [
    "A common way to create constant tensors is via tf.ones and tf.zeros (just like np.ones and np.zeros):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76e3ea65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1.]\n",
      " [1.]], shape=(2, 1), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[0.]\n",
      " [0.]], shape=(2, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(tf.ones(shape=(2, 1)))\n",
    "print(tf.zeros(shape=(2, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7229a71",
   "metadata": {},
   "source": [
    "You can also create random constant tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0533ce5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.random.normal(shape=(2, 2), mean=0.0, stddev=1.0)\n",
    "\n",
    "x = tf.random.uniform(shape=(2, 2), minval=0, maxval=10, dtype=\"int32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10db70e1",
   "metadata": {},
   "source": [
    "## Variables\n",
    "\n",
    "Variables are special tensors used to store mutable state (such as the weights of a neural network). You create a Variable using some initial value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c90c3660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(2, 2) dtype=float32, numpy=\n",
      "array([[ 1.6822869 ,  0.5285346 ],\n",
      "       [-0.19078438,  0.78927237]], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "initial_value = tf.random.normal(shape=(2, 2))\n",
    "a = tf.Variable(initial_value)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90b3609",
   "metadata": {},
   "source": [
    "You update the value of a Variable by using the methods ```.assign(value)```, ```.assign_add(increment)```, or ```.assign_sub(decrement)```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e2972f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_value = tf.random.normal(shape=(2, 2))\n",
    "a.assign(new_value)\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        assert a[i, j] == new_value[i, j]\n",
    "\n",
    "added_value = tf.random.normal(shape=(2, 2))\n",
    "a.assign_add(added_value)\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        assert a[i, j] == new_value[i, j] + added_value[i, j]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cb6411",
   "metadata": {},
   "source": [
    "## Doing math \n",
    "\n",
    "If you've used NumPy, doing math in TensorFlow will look very familiar. The main difference is that your TensorFlow code can run on GPU and TPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2f21eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.random.normal(shape=(2, 2))\n",
    "b = tf.random.normal(shape=(2, 2))\n",
    "\n",
    "c = a + b\n",
    "d = tf.square(c)\n",
    "e = tf.exp(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746f28f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d949d7a1",
   "metadata": {},
   "source": [
    " # 2. Building a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f9350c",
   "metadata": {},
   "source": [
    "## 2.1. Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587776dc",
   "metadata": {},
   "source": [
    "\n",
    "In this tutorial we will see how to use the Keras frmaework to implement a multiclass classifier on a popular dataset called MNIST.  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076fcb3c",
   "metadata": {},
   "source": [
    "[MNIST](http://yann.lecun.com/exdb/mnist/) dataset has 70k small grayscale images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e2dfaa",
   "metadata": {},
   "source": [
    "![MNIST](imgs/mnist.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4086f1",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "38aa4128",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a9eb92",
   "metadata": {},
   "source": [
    "## 2.2. Build the model¬∂"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ccc6be",
   "metadata": {},
   "source": [
    "To build our model in keras, we use the [tf.keras.Sequential](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential) model by stacking layers. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bf96ac34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e46a63",
   "metadata": {},
   "source": [
    "Define the model: Choose network architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c23d0d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)), # Images are 28x28 px\n",
    "  tf.keras.layers.Dense(64, activation='relu'), # Hidden layer\n",
    "  tf.keras.layers.Dense(64, activation='relu'),                  # Normalization layer\n",
    "  tf.keras.layers.Dense(10, activation='softmax')# There are 10 classes\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ef3f9a",
   "metadata": {},
   "source": [
    "First layer takes in 28x28, because our images are 28x28 images of hand-drawn digits. A basic neural network is going to expect to have a flattened array, so not a 28x28, but instead a 1x784.\n",
    "\n",
    "We used relu(rectified linear unint) as activation function. \n",
    "\n",
    "Basically, these activation functions are keeping our data scaled between 0 and 1.\n",
    "\n",
    "Finally, for the output layer, we used softmax. Softmax makes sense to use for a multi-class problem, where each thing can only be one class or the other. This means the outputs themselves are a confidence score, adding up to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f716f82d",
   "metadata": {},
   "source": [
    "There are many rule-of-thumb methods for determining the correct number of neurons to use in the hidden layers, such as the following:\n",
    "- The number of hidden neurons should be between the size of the input layer and the size of the output layer.\n",
    "- The number of hidden neurons should be 2/3 the size of the input layer, plus the size of the output layer.\n",
    "- The number of hidden neurons should be less than twice the size of the input layer.\n",
    "\n",
    "Moreover, the number of neurons and number layers required for the hidden layer also depends upon training cases, the complexity of, data that is to be learned, and the type of activation functions used.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49292c1",
   "metadata": {},
   "source": [
    "## 2.3. Compile model: \n",
    "\n",
    "Choose optimizer, loss function, and optionally a monitoring metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "53b3097d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate= 0.001),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f63edf",
   "metadata": {},
   "source": [
    "* Our ```loss_function``` is what calculates \"how far off\" our classifications are from reality.\n",
    "It is a measurement of how far off the neural network is from the targeted output.\n",
    "\n",
    "* ```optimizer``` adjusts our model's adjustable parameters like the weights, to slowly, over time, fit our data.\n",
    "\n",
    "* The learning rate dictates the magnitude of changes that the optimizer can make at a time. Thus, the larger the LR, the quicker the model can learn, but also you might find that the steps you allow the optimizer to make are actually too big and the optimizer gets stuck bouncing around rather than improving. Too small, and the model can take much longer to learn as well as also possibly getting stuck. Indeed a too small value will require a very large number of epochs to converge while the algorithm might not converged by setting a too large value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef389580",
   "metadata": {},
   "source": [
    "![LR](https://deeplearningmath.org/images/learning_rate_choice.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2961eea0",
   "metadata": {},
   "source": [
    "Moreover, it is not recommended to use a constant learning rate. Indeed, even if a large value can help the algorithm to arrive quickly to a good solution, then it might oscillate around this state for a long time or diverge if the learning rate is maintained. A solution is to allow the learning rate to decay over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4f8b3d",
   "metadata": {},
   "source": [
    "> For simpler tasks, a learning rate of 0.001 usually is more than fine. For more complex tasks, you will see a learning rate with what's called a decay. Basically you start the learning rate at something like 0.001, or 0.01...etc, and then over time, that learning rate gets smaller and smaller. The idea being you can initially train fast, and slowly take smaller steps, hopefully getthing the best of both worlds.\n",
    "\n",
    "A common approach is to half the learning rate every 5 epochs, or by 0.1 every 20 epochs. A proposed heuristic is to track the validation error while training with a fixed learning rate, and if the validation error stops improving then reduce the learning rate by a constant (e.g. 0.5)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ba46a9",
   "metadata": {},
   "source": [
    "## 2.4. Training the network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f28ab5",
   "metadata": {},
   "source": [
    "### Epoch\n",
    "\n",
    "![Epoch](https://miro.medium.com/max/1024/1*cDhZ56QNC5mrl6kjE0C2JA.png)\n",
    "\n",
    "In deep learning an epoch is a [hyperparameter](https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning)) which is defined before training a model. In other words, one epoch is when an entire dataset is passed both forward and backward through the neural network only once.\n",
    "\n",
    "The reason why we have to split the training step by epochs is decrease the amount of data we feed to the computer at once. So, we divide it in several smaller batches. \n",
    "\n",
    "We use more than one epoch because passing the entire dataset through a neural network is not enough and we need to pass the full dataset multiple times to the same neural network. But since we are using a limited dataset we can do it in an iterative process. A batch is the total number of training examples present in a single batch and an iteration is the number of batches needed to complete one epoch.\n",
    "\n",
    "**Example**: \n",
    "\n",
    "If we divide a dataset of 2000 training examples into 500 batches, then 4 iterations will complete 1 epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2b866f",
   "metadata": {},
   "source": [
    "* Too few epochs, and your model wont learn everything it could have.\n",
    "\n",
    "* Too many epochs and your model will over fit to your in-sample data (basically memorize the in-sample data, and perform poorly on out of sample data).\n",
    "\n",
    "Let's go with 5 epochs for now. So we will loop over epochs, and each epoch will loop over our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e5777b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 1.5331 - accuracy: 0.7915\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.3958 - accuracy: 0.9031\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.3281 - accuracy: 0.9187\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.2913 - accuracy: 0.9273\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.2521 - accuracy: 0.9363\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x172988490>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7403cf0f",
   "metadata": {},
   "source": [
    "## 2.5. Test the network on the test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "755540e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate on test data\n",
      "79/79 [==============================] - 0s 2ms/step - loss: 0.2707 - accuracy: 0.9327\n",
      "test loss, test acc: [0.27070385217666626, 0.932699978351593]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test data using `evaluate`\n",
    "print(\"Evaluate on test data\")\n",
    "results = model.evaluate(X_test, y_test, batch_size=128)\n",
    "print(\"test loss, test acc:\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23106655",
   "metadata": {},
   "source": [
    "The image classifier is now trained to ~94% accuracy on this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0cf1fd8",
   "metadata": {},
   "source": [
    "## 2.6.  Saving the model\n",
    "\n",
    "Model training usually takes a lot of time, so once the model is trained it is smart to save it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a83d143",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('Models/mnist.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22465b9",
   "metadata": {},
   "source": [
    "## üèÅ 3. Conclusion\n",
    "\n",
    "Now, you know:\n",
    "\n",
    "1. a popular deep learning framework: Keras,\n",
    "2. the basic building blocks of deep learning,\n",
    "3. how to load data and define a Neural Network, \n",
    "4. how to train and test a Neural Network using Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6db06d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
