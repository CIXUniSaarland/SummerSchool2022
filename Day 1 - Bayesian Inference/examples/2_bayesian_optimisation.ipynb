{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import scipy.stats as ss\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# set some styling defaults for matplotlib\n",
    "plt.style.use(\"seaborn-talk\")\n",
    "mpl.rcParams[\"figure.dpi\"] = 90  # change this to set apparent figure size\n",
    "mpl.rcParams[\"figure.figsize\"] = (7, 3)\n",
    "mpl.rcParams[\"figure.frameon\"] = False\n",
    "\n",
    "# set decimal precision to 3 dec. places\n",
    "%precision 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install scikit-optimize\n",
    "# uncomment and run the above if you don't have skopt installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skopt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian optimisation\n",
    "\n",
    "## Outcomes\n",
    "\n",
    "* How Bayesian optimisation works\n",
    "* How to build proxy functions \n",
    "* How to choose acquisition functions \n",
    "* How to apply BO to problems of tuning interfaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal\n",
    "* Formulate a problem as a mapping from parameters (things we can adjust) to an objective (a single numerical measure of goodness) \n",
    "    * This is an \"objective function\" \n",
    "* Optimise (i.e. tweak parameters) to maximise the objective function; this should maximise the goodness of our configuration.\n",
    "* *Do this without knowing what the objective function is!*\n",
    "* Instead, make measurements of the real world for a few specific, definite values of the parameters and optimise based on a (distribution over) *proxy objective function*\n",
    "* We want to do this such that we:\n",
    "    * Spend the least time possible making measurements.\n",
    "    * Cope with the face that the real world is noisy.\n",
    "\n",
    "## Task\n",
    "We will model a problem of optimising reading speed on a mobile device. There are many things we can alter in page layout: margins, font size, font shape, line spacing, etc. We don't have a good model of what effect these have on reading speed, but if we're building a device that is supposed to help users read quickly (note that this is a very narrow choice of optimality!) then we need to establish one.\n",
    "\n",
    "<img src=\"reading_kindle.png\">\n",
    "\n",
    "### Optimising font size\n",
    "We'll stick to just one variable for the moment, *font size*\n",
    "\n",
    "Our problem can be written as:\n",
    "\n",
    "* minimise reading time for a given text and font size\n",
    "\n",
    "Formal optimisation would express this as:\n",
    "\n",
    "$$\\theta^* = \\operatorname{argmin}_\\theta f(x;\\theta)\\ \\text{s.t.}\\ c(\\theta)>0$$\n",
    "\n",
    "$\\theta^*$ is the configuration we're looking for; $x$ is the text the reader is reading; and $\\theta$ is the parameters, which in this case is just one number: font size. We also have some basic constraints: font size must be positive and less than some maximum font size; $c(\\theta)$ represents this constraint in the equation above.\n",
    "\n",
    "We don't know what $f(x;\\theta)$ is *but* we can evaluate it pointwise: choose a specific $x$ and a specific $\\theta$ (i.e. a text and a font size), give it to a user, and ask them to read the text. We time how long they take, and that is the measure at that specific instant. Obviously, this is a bit noisy, but that's okay.\n",
    "\n",
    "## Process\n",
    "\n",
    "* Create a prior distribution over proxy objective function (i.e. guesses for what $f(x;\\theta)$ might look like)\n",
    "* Decide on a rule to choose the next experiment to run. This is given by the **acquisition function**, which outputs a new $\\theta$ given a distribution over $f(x;\\theta)$.\n",
    "* Apply the rule to choose a specific $\\theta$, run the experiment, update the distribution to get a posterior.\n",
    "\n",
    "We'll use a flexible class of models called **Gaussian Processes**: the details don't really matter here, but they give a really simple way to form distributions over continuous functions even when we know very little about their form of those functions. They are \"properly\" probabilistic, and inferring posteriors (over functions) given definite but noisy observations is a standard operation.\n",
    "\n",
    "## Package: `skopt`\n",
    "`skopt` is a basic package that makes it easy to implement basic Bayesian optimisation. It gives us the ability to specify distributions over functions as GPs, and then a selection of acquisition functions to decide on the next optimal point to choose.\n",
    "\n",
    "\n",
    "## Setup\n",
    "We don't have enough time to run real experiments. So I've created a simulator -- a black box -- that will simulate reading documents. It takes a text and a font size and returns the number of seconds to read that text on a small mobile screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simulate_reading import read_text, word_count\n",
    "\n",
    "# return value is in seconds\n",
    "read_text(\"The long brown cat jumped over the slippery dog\", font_size=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"war_peace.txt\", encoding=\"utf8\") as f:\n",
    "    war_peace = f.read()\n",
    "\n",
    "read_text(war_peace)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_text(war_peace, font_size=16)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_text(war_peace, font_size=4)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def war_peace_wps(font_size, portion=1.0):\n",
    "    # normalise to be seconds/word\n",
    "    text = war_peace[:int(len(war_peace)*portion)]\n",
    "    time = read_text(text, font_size)\n",
    "    wc = word_count(text)\n",
    "    return time / wc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A simple Gaussian Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt.learning.gaussian_process import GaussianProcessRegressor\n",
    "from skopt.learning.gaussian_process.kernels import RBF\n",
    "\n",
    "import scipy.stats\n",
    "from gp_utils import render_gp, ei, pi, lcb, plot_gp_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling\n",
    "\n",
    "A Gaussan Process (GP) is a distribution over functions, so we can sample from it, to get plausible functions (curves)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "font_sizes = np.linspace(1, 28, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_gp = GaussianProcessRegressor(kernel = RBF(3.0, \"fixed\"))\n",
    "ys = basic_gp.sample_y(font_sizes[:, None], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plot_gp_samples(ax, font_sizes, ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_gp = GaussianProcessRegressor(kernel = RBF(10.0, \"fixed\"))\n",
    "ys = basic_gp.sample_y(font_sizes[:, None], 10)\n",
    "fig, ax = plt.subplots()\n",
    "plot_gp_samples(ax, font_sizes, ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution over functions\n",
    "We have a whole distribution over functions, though it is very boring looking since we're averageing over *every* possible smooth function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "render_gp(ax, font_sizes, basic_gp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A data point\n",
    "We can perform Bayesian inference, by computing the likelihood of an observation under every possible function, and then reweighting to place more density on those functions that are compatible, exactly as we have done before.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "font_size = 12\n",
    "t_12 = war_peace_wps(font_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "font_gp = GaussianProcessRegressor(kernel = RBF(10.0, \"fixed\"))\n",
    "font_gp.fit([[font_size]], [t_12])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "render_gp(ax, font_sizes, font_gp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Noise\n",
    "Typically, running a human subject experiment doesn't give a definite answer. Instead, we'd be more cautious and assume that the data generating process produced a reading time *plus some noise*. We can easily configure the GP to account for our (estimated) noise by setting the $\\alpha$ parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with some uncertainty on outputs\n",
    "font_gp_noise = GaussianProcessRegressor(kernel = RBF(5.0, \"fixed\"), alpha=0.05)\n",
    "font_gp_noise.fit(np.array([[font_size]]), np.array([t_12]))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "render_gp(ax, font_sizes, font_gp_noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acquisition function\n",
    "We now use an acquisition function to obtain a definite point to measure (i.e. a new experiment to run), given a distribution over functions.\n",
    "\n",
    "We could work out what value would give us the largest average increase (**expected improvement** or EI) in our estimate. We can also compute the **probability of improvement** (PI) and the **lower confidence bound** (LCB). The choice between these doesn't make a huge difference in our example, but they imply slightly different choices for the search process. In each case, we evaluate the AF across the domain, and choose the optimal value as the next point to test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=4, figsize=(12, 12))\n",
    "render_gp(ax[0], font_sizes, font_gp_noise)\n",
    "\n",
    "y_mean, y_std = font_gp_noise.predict(font_sizes[:, None], return_std=True)\n",
    "exp_improvement = ei(y_mean, y_std)\n",
    "ax[1].plot(exp_improvement, c='k')\n",
    "ax[1].set_ylabel(\"EI\")\n",
    "\n",
    "\n",
    "pred_improvement = pi(y_mean, y_std)\n",
    "ax[2].plot(pred_improvement, c='k')\n",
    "ax[2].set_ylabel(\"PI\")\n",
    "\n",
    "\n",
    "lcb_improvement = lcb(y_mean, y_std, kappa=1)\n",
    "ax[3].plot(lcb_improvement, c='k')\n",
    "ax[3].set_ylabel(\"LCB\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying\n",
    "Now we can run this in a loop -- at each point, we choose a point than optimises an acquisition function, then run an experiment (calculate reading time) at that point.\n",
    "\n",
    "We make our subjects read the *whole* of War and Peace -- this reduces variation, but increases the time for each acquisition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_predict_gp(xs, ys, noise):\n",
    "    gp = GaussianProcessRegressor(kernel=RBF(5.0, \"fixed\"), alpha=noise)\n",
    "    if len(xs)>0:\n",
    "        gp.fit(np.array(xs).reshape(-1, 1), np.array(ys))\n",
    "    y_pred, y_std = gp.predict(font_sizes.reshape((-1, 1)), return_std=True)\n",
    "    return y_pred, y_std, gp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fonts = []\n",
    "test_times = []\n",
    "\n",
    "for i in range(10):\n",
    "    # fit GP    \n",
    "    y_pred, y_std, gp = fit_predict_gp(test_fonts, test_times, noise=0.01)\n",
    "    # use lower confidence bound\n",
    "    exp_imp = lcb(y_pred, y_std, kappa=4) \n",
    "    # find biggest improvement    \n",
    "    next_font = font_sizes[np.argmin(exp_imp)]    \n",
    "    # perform the experiment\n",
    "    time = war_peace_wps(next_font)\n",
    "    # plot the curve\n",
    "    fig, ax = plt.subplots()    \n",
    "    render_gp(ax, font_sizes, gp)\n",
    "    ax.scatter(test_fonts, test_times, c=\"r\")\n",
    "    ax.plot(font_sizes, exp_imp, c='r')\n",
    "    \n",
    "    # store the result\n",
    "    test_fonts.append(next_font)\n",
    "    test_times.append(time)\n",
    "\n",
    "    \n",
    "ax.scatter(test_fonts, test_times, c=\"r\")    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_fonts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More noise\n",
    "We could ask subjects to just read the first ten or so pages, and extrapolate from there. This is quick, but noisier -- but we can easily accommodate that in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fonts = []\n",
    "test_times = []\n",
    "\n",
    "for i in range(12):\n",
    "    # fit GP    \n",
    "    y_pred, y_std, gp = fit_predict_gp(test_fonts, test_times, noise=0.1)\n",
    "    # use lower confidence bound\n",
    "    exp_imp = lcb(y_pred, y_std, kappa=4) \n",
    "    # find biggest improvement    \n",
    "    next_font = font_sizes[np.argmin(exp_imp)]    \n",
    "    # perform the experiment\n",
    "    time = war_peace_wps(next_font, portion=0.01)\n",
    "    # plot the curve\n",
    "    fig, ax = plt.subplots()    \n",
    "    render_gp(ax, font_sizes, gp)\n",
    "    ax.scatter(test_fonts, test_times, c=\"r\")\n",
    "    ax.plot(font_sizes, exp_imp, c='r')\n",
    "    \n",
    "    # store the result\n",
    "    test_fonts.append(next_font)\n",
    "    test_times.append(time)\n",
    "\n",
    "    \n",
    "ax.scatter(test_fonts, test_times, c=\"r\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes: automatic Bayesian optimisation\n",
    "Note that I've manually executed this loop to show you the process in detail. `skopt` would normally be used in an automatic mode, where it would optimise sequentially without intervention. It can do clever things, like take into account the expected time to run an experiment, or hedge against a bad choice of acquisition function by optimising that choice online as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = skopt.gp_minimize(lambda x:war_peace_wps(x[0], portion=0.05),\n",
    "                  [(1.0, 30.0)], # range\n",
    "                  acq_func='LCB', # acquisition function\n",
    "                  n_initial_points = 1, # one initial random guess\n",
    "                  n_calls = 10, # 10 iterations\n",
    "                 noise=0.1) # noise level\n",
    "print(result.x_iters)              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflection\n",
    "\n",
    "### What did we gain?\n",
    "* We have an efficient way to optimise parameters of an interface when it costs a lot to obtain real data. \n",
    "* We can deal with inaccurate or noisy measurements; we won't be misled by an occasional bad data point\n",
    "* We *know how certain we are* -- whether or not it is worth continuing to run experiments or not.\n",
    "* We only had to make very weak assumptions about the objective function: no complicated modelling required.\n",
    "\n",
    "### What caused trouble?\n",
    "* There are still choices to make about the appropriate acquisition function to choose\n",
    "* and about the right \"smoothness\" of the proxy objective function.\n",
    "* It's not necessarily obvious how to bring in other expert knowledge about the objective function (e.g. is always positive, monotonic, quadratic, or whatever).\n",
    "* BO works well in small dimensions, but it isn't very efficient where there are many parameters to optimise at once.\n",
    "* This isn't easy to apply to tasks like layout, ranking, etc. where discrete optimisation is required.\n",
    "\n",
    "### What else could we do?\n",
    "* We only optimised in one dimension here; realistically,  we'd be likely to be optimising over a few dimensions at once.\n",
    "* We only considered pure sequential experiments. We can also compute the acquisition function for multiple simultaneous experiments; though the computation becomes more intensive if we want to do this optimally."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ace11ca1210033961de0244ddbce58b4d2e3f6fcf9cb204c358c638083cc5c0f"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
