{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive and Explainable AI \n",
    "\n",
    "Here, we will study the implementation of tools for interactive GAN exploration by Zhang and Banovic (2021) that we covered in the first part of the lecture. However, this is by no means an optimized version; here, we are happy to tradeoff performance for even a smallest gain in clarity. Also note that this notebook is <strong>not</strong> the full implementation of GAN Explorer from that paper. To implement the full web-based tool would require placing the code from this notebook onto a Webserver, connecting it with a database that will permantenly store the data for each individual user, and implementing client side user interface.\n",
    "\n",
    "With that said, let's start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For utility operations\n",
    "import sys\n",
    "import os \n",
    "\n",
    "# For vectror and matrix operations, and sampling.\n",
    "import math\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "import random\n",
    "from scipy.stats import truncnorm\n",
    "\n",
    "# For drawing images and graphs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# BigGAN model\n",
    "import torch\n",
    "import nltk\n",
    "from pytorch_pretrained_biggan import (BigGAN, BigGANConfig, one_hot_from_names, truncated_noise_sample, save_as_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emptiest_gpu():\n",
    "    # Command line to find memory usage of GPUs. Return the one with most mem available.\n",
    "    output = os.popen('nvidia-smi -q -d Memory | grep -A4 GPU | grep Free').read().strip()\n",
    "    mem_avail = [int(x.split()[2]) for x in output.split('\\n')]\n",
    "    return mem_avail.index(max(mem_avail))\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.set_device(get_emptiest_gpu())\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "For our interactive model exploration, we will be using a pre-trained BigGAN model. For more information about the API that we will use to access the model see: https://github.com/huggingface/pytorch-pretrained-BigGAN.\n",
    "\n",
    "Remember from the lecture:\n",
    "\n",
    "- Training a GAN involves two networks:\n",
    "    - a Generator that takes in a vector of latent variables $z = (ð‘§_1, ..., ð‘§_ð‘›) âˆˆ R^ð‘›$ and outputs the corresponding image\n",
    "    - a Discriminator that is used to distinguish between generated images and real images (training data).\n",
    "\n",
    "- The Generator is trained to maximize the probability of fooling the Discriminator\n",
    "\n",
    "- The Discriminator is trained to discriminate training data from the images created by the Generator.\n",
    "\n",
    "Here, we will only be focusing on the generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = BigGAN.from_pretrained('biggan-deep-128')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Model Output\n",
    "Now that we have the model, it is time to think about how to assess its capabilities and limitations. One way to explore the model is through qualitative model inspection.\n",
    "\n",
    "Take a moment to think what is the most basic interaction that we can implement to explore the model?\n",
    "\n",
    "One such interaction is to visualize the outputs of the model as an image gallery! The function below takes as input a model, a vector of model parameters ($z$), and corresponding image categories, then outputs an image gallery containing images that correspond to those model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_gallery(biggan_G, z_s, categories, rows=5, cols=5, truncation=1):\n",
    "    \n",
    "    if rows == 0 or cols == 0:\n",
    "        return\n",
    "    \n",
    "    fig, axs = plt.subplots(rows, cols, figsize=(cols*2, rows*2))\n",
    "\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            _z = torch.from_numpy(z_s[i * rows + j]).type(torch.float).reshape(1, -1)\n",
    "            _y = torch.tensor([categories[i * rows + j]])\n",
    "            _z = _z.to(device)\n",
    "            _y = _y.to(device)\n",
    "            output_img_org = biggan_G(_z, _y, truncation)\n",
    "            output_img = (output_img_org - output_img_org.min()) / (output_img_org.max() - output_img_org.min())\n",
    "            display_img = output_img.squeeze(0).permute(1, 2, 0).cpu().detach().numpy()\n",
    "            display_img = (display_img * 255).astype(np.uint8)\n",
    "            if rows > 1:\n",
    "                axs[i, j].imshow(display_img)\n",
    "                axs[i, j].axis(\"off\")   # turns off axes\n",
    "                axs[i, j].axis(\"tight\")  # gets rid of white border\n",
    "            elif rows == 1 and cols == 1:\n",
    "                # Single image \"gallery\".\n",
    "                axs.imshow(display_img)\n",
    "                axs.axis(\"off\")   # turns off axes\n",
    "                axs.axis(\"tight\")  # gets rid of white border\n",
    "            else:\n",
    "                # Single row gallery.\n",
    "                axs[j].imshow(display_img)\n",
    "                axs[j].axis(\"off\")   # turns off axes\n",
    "                axs[j].axis(\"tight\")  # gets rid of white border\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the cell below a few times (each time you should get a different output). Feel free to change the parameters (e.g., the truncation, the names of image categories that you want to generate).\n",
    "\n",
    "Answer these questions:\n",
    "- What happens as you increase truncation up to 1?\n",
    "- What happens as you reduce truncation down to 0?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truncation = 0.5\n",
    "class_vector = one_hot_from_names(['indigo finch','irish setter','dome','headland','cup']*5, batch_size=25)\n",
    "z_vectors = truncated_noise_sample(truncation=truncation, batch_size=25)\n",
    "display_gallery(generator, z_vectors, class_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may say that such an image gallery is not very interactive; and you'd be right. Though, that is what most model designers currently use when investigating their GANs.\n",
    "\n",
    "Let's see if we can make our image gallery more interactive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive GAN Model Exploration\n",
    "Here, we will explore the concept of an image gallery that will provide the user with a \"lens\" onto our model. Our image gallery displays $n \\times n$ GAN-generated images organized in a grid. The user can then interact with the image gallery to \"move\" this lens from one region of the model to another to map out regions where the model generates high-quality images and other regions where the quality of images is not as good.\n",
    "\n",
    "Here, we draw inspiration from existing work (Koyama et al., 2020) to mathematically formalize the current working gallery as a square region of a 2D plane $\\mathcal{P}$. We uniquely define plane region $\\mathcal{P}$ in a hyperspace with three vectors $\\mathbf{c}$, $\\mathbf{u}$, $\\mathbf{v}$, where $\\mathbf{c}$ is the center of the region of the plane, and $\\mathbf{u}$ and $\\mathbf{v}$ are two orthogonal vectors with equal length on the plane that both point from the center $\\mathbf{c}$.\n",
    "\n",
    "Note that the number of dimensions of each vector $\\mathbf{c}$, $\\mathbf{u}$, $\\mathbf{v}$ corresponds to the number of dimensions of the vector of latent variables $\\mathbf{z}$ that the GAN takes as input. We then represent the images in the current working gallery as $n$ equally-spaced data points on the plane region, denoted by a set of vectors of latent variables $Z=\\{\\mathbf{z_1,z_2,...,z_{n}}\\}$, where each vector of latent variables $\\mathbf{z_i}\\in{Z}$ is an input to the GAN model with the corresponding image as output.\n",
    "\n",
    "Image below shows an example square plane region $\\mathcal{P}$ containing twenty five equally-spaced data points (denoted by a set of vectors of latent variables $Z=\\{\\mathbf{z_1,z_2,...,z_{25}}\\}$).\n",
    "\n",
    "<div><img src='img/plane-diagram.png' width='40%'/></div>\n",
    "\n",
    "Let's initialize one such image gallery and see how that looks in the code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling_from_plane(c, u, v, num_dims = 10, rows=5, cols=5, base=10):\n",
    "    '''\n",
    "    Return rows x cols sampling points from the given plane, paremeterized by (c,u,v).\n",
    "    \n",
    "    Args:\n",
    "        c: center point of the plane.\n",
    "        u: vector u.\n",
    "        v: vector v, which is orthogonal to u.\n",
    "\n",
    "    Returns:\n",
    "        sampling_points: shape: (rows x cols). Each row is a sampling point.\n",
    "    '''\n",
    "    vec_len = base / (1.2 ** zoom_level)\n",
    "    \n",
    "    u = vec_len * u / np.linalg.norm(u)\n",
    "    v = vec_len * v / np.linalg.norm(v)\n",
    "    \n",
    "    c = c.reshape(-1)\n",
    "    u = u.reshape(-1)\n",
    "    v = v.reshape(-1)\n",
    "    \n",
    "    row_step = (v - u) / 4\n",
    "    col_step = (-1 * v - u) / 4\n",
    "    start_point = c - 2 * row_step - 2 * col_step\n",
    "    sampling_points = np.zeros((rows * cols, num_dims))\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            sampling_points[i * rows + j] = start_point + row_step * i + col_step * j\n",
    "    return sampling_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_z_vectors(sampling_points, base_vector, tune_idxs, rows = 5, cols = 5):\n",
    "    z_vectors = []\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            real_point = np.copy(base_vector)\n",
    "            real_point[tune_idxs] = sampling_points[i * rows + j]\n",
    "            z_vectors.append(real_point)\n",
    "    return np.asarray(z_vectors)\n",
    "\n",
    "def compute_sampling_points(z_vectors, tune_idxs, rows = 5, cols = 5):\n",
    "    return np.apply_along_axis((lambda x: x[tune_idxs]), 1, z_vectors)\n",
    "\n",
    "def get_index_of(row, col, rows=5, cols=5):\n",
    "    return (row-1) * rows + (col - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_len = generator.config.z_dim\n",
    "zoom_level=0 \n",
    "num_dims = 10\n",
    "iteration = 0\n",
    "tune_idxs = np.random.choice(z_len, num_dims, replace=False)\n",
    "tune_idxs.sort()\n",
    "\n",
    "category = 'indigo finch'\n",
    "\n",
    "# Saved snapshots saved by category and name.\n",
    "snapshots = {}\n",
    "\n",
    "# Lists of user selected samples keyed by category.\n",
    "select_samples = {}\n",
    "\n",
    "class_vector = one_hot_from_names([category]*25, batch_size=25)\n",
    "\n",
    "# Initialize plane P_1\n",
    "# Always start with the \"most representative\" example of the class in the center.\n",
    "base_vector = np.zeros(z_len)\n",
    "\n",
    "c = base_vector[tune_idxs]\n",
    "\n",
    "v_subset = np.random.randn(num_dims)\n",
    "v_subset /= np.linalg.norm(v_subset)\n",
    "\n",
    "u_subset = np.random.randn(num_dims)\n",
    "u_subset -= u_subset.dot(v_subset) * v_subset\n",
    "u_subset /= np.linalg.norm(u_subset)\n",
    "\n",
    "sampling_points = sampling_from_plane(c, u_subset, v_subset)\n",
    "\n",
    "z_vectors = compute_z_vectors(sampling_points, base_vector, tune_idxs)\n",
    "\n",
    "display_gallery(generator, z_vectors, class_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our image gallery, let's implement some interactions..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomize\n",
    "Randomizing the current image gallery will simply generate a new working gallery. This is similar to how existing methods generate image galleries and it is not too exciting. However, it is still a useful interaction because it allows the user to start their exploration in a new region of the model that they can stumble upon serendipitously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomize(truncation=0.04, rows=5, cols=5):\n",
    "    '''\n",
    "    Reinitializes the current plane segment by randomly selecting a center point and recomputing the other vectors.\n",
    "    truncation - how far from the most representative sample can the central image be.\n",
    "    \n",
    "    rows - used to calculate the number of z_vectors to compute.\n",
    "    \n",
    "    cols - used to calculate the number of z_vectors to compute.\n",
    "    \n",
    "    Returns a randomly selected z_vectors of size rows x cols.\n",
    "    '''\n",
    "\n",
    "    c = truncated_noise_sample(truncation=truncation, batch_size=1)[0][tune_idxs]\n",
    "    \n",
    "    v_subset = np.random.randn(len(tune_idxs))\n",
    "    v_subset /= np.linalg.norm(len(tune_idxs))\n",
    "    \n",
    "    u_subset = np.random.randn(len(tune_idxs))\n",
    "    u_subset -= u_subset.dot(v_subset) * v_subset\n",
    "    u_subset /= np.linalg.norm(u_subset)\n",
    "\n",
    "    sampling_points = sampling_from_plane(c, u_subset, v_subset, len(tune_idxs), rows, cols)\n",
    "    \n",
    "    z_vectors = compute_z_vectors(sampling_points, base_vector, tune_idxs)\n",
    "    \n",
    "    return z_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what what kind of galleries we can find."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_vectors = randomize()\n",
    "display_gallery(generator, z_vectors, class_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the code above a few times and stop when you see a gallery with at least one image that you think is photo-realistic. Feel free to play with the truncation parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pivot\n",
    "Simply restarting the exploration from a random working gallery is not always desirable as it may prevent explorations near the current search space region. To serendipitously explore around the current region of the model, we can \"pivot\" the current image gallery.\n",
    "\n",
    "As mentioned earlier, there are 25 images in the current working gallery (each corresponding to a point on the $Z=\\{\\mathbf{z_1,z_2,...,z_{n}}\\}$ in the current plane region $\\mathcal{P}$). Each $\\mathbf{z_i}\\in Z$ has a corresponding vector of latent variables with $m$ dimensions (in our example $n=128$).\n",
    "\n",
    "We \"pivot\" by fixing a subset of dimensions in all $n$ latent vectors and changing the values of the remaining dimensions, which results in a new, shifted plane region $\\mathcal{P'}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pivot(truncation=0.2, num_dims = 10, rows=5, cols=5):\n",
    "    # Freeze tuning features in sampling points\n",
    "    sampling_points = compute_sampling_points(z_vectors, tune_idxs)\n",
    "    \n",
    "    # Randomize base vector\n",
    "    base_vector[:] = truncated_noise_sample(truncation=truncation, batch_size=1)[0]\n",
    "    \n",
    "    new_z_vectors = compute_z_vectors(sampling_points, base_vector, tune_idxs)\n",
    "    \n",
    "    return new_z_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_vectors = pivot()\n",
    "display_gallery(generator, z_vectors, class_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that despite its user-friendly name that describes the appearance of the effect in the resulting gallery, mathematically, this tool actually shifts the resulting plane region."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pan\n",
    "Up until this point, all of the interactions that we introduced were stochastic --- their output is quite unpredictable. Although this aids in serendipitous exploration of the model, the user might want to have more control over exploring specific regions of the model. \n",
    "\n",
    "Note that the current working gallery only shows a region of the current plane 2D, but the plane extends in all four directions. This gives rise to the pan interaction.\n",
    "\n",
    "Here is the math behing panning to the left (without loss of generality). Let the current working gallery contain $n$ images organized in a $n_{rows} \\times n_{cols}$ grid as a set of vectors of latent variables $Z=\\{\\mathbf{z_1,z_2,...,z_{n}}\\}$ on the current plane region $\\mathcal{P}$ defined by vectors $\\mathbf{c,u,v}$. Let $i$ be the index of the latent vector $z_i$ in the center of the gallery (i.e., $z_i = c$). Then, we can define the following two direction vectors:\n",
    "\n",
    "1) vector $\\mathbf{a}$ points from the top of the gallery to the bottom of the gallery, and\n",
    "\n",
    "2) vector $\\mathbf{b}$ points from gallery's left to gallery's right.\n",
    "\n",
    "\n",
    "Now, we can compute the resulting plane region $\\mathcal{P'}$ defined by $(\\mathbf{c',u',v'})$ as:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathbf{c'}=\\mathbf{z_{i-1}}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathbf{u'}=2(-\\mathbf{a}-\\mathbf{b})\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathbf{v'}=2(\\mathbf{a}-\\mathbf{b})\n",
    "\\end{equation}\n",
    "\n",
    "Similarly, it follows that we can pan in the other three directions using a similar way to construct the resulting plane region of the resulting working gallery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pan(z_vectors, direction, truncation=0.04, num_dims = 10, rows=5, cols=5):\n",
    "    sampling_points = compute_sampling_points(z_vectors, tune_idxs)\n",
    "    \n",
    "    row_step = sampling_points[rows] - sampling_points[0]\n",
    "    col_step = sampling_points[1] - sampling_points[0]\n",
    "    \n",
    "    center_idx = int(rows * cols / 2)\n",
    "    new_center_idx = center_idx\n",
    "    \n",
    "    if direction == \"up\":\n",
    "        new_center_idx = center_idx - cols \n",
    "    elif direction == \"down\":\n",
    "        new_center_idx = center_idx + cols \n",
    "    elif direction == \"left\":\n",
    "        new_center_idx = center_idx - 1\n",
    "    elif direction == \"right\":\n",
    "        new_center_idx = center_idx + 1\n",
    "\n",
    "    c = sampling_points[new_center_idx]\n",
    "    \n",
    "    u = -2*row_step-2*col_step\n",
    "    v = 2*row_step-2*col_step\n",
    "    \n",
    "    sampling_points = sampling_from_plane(c, u, v) \n",
    "    z_vectors = compute_z_vectors(sampling_points, base_vector, tune_idxs)\n",
    "    \n",
    "    return z_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_vectors = pan(z_vectors, \"left\")\n",
    "display_gallery(generator, z_vectors, class_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the code above a few times. Change the direction of pan and stop when you see a gallery with at least one new image that you think is photo-realistic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Gallery Snapshots\n",
    "To support user control and freedom, the user should be able to save the current working gallery to restart their exploration from that point at a later time. Here, we provide two snapshot functions to create a new snapshop and revert to an old snapshot. Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_snapshot(category, name, z_vectors):\n",
    "    if category not in snapshots:\n",
    "        snapshots[category] = {}\n",
    "        \n",
    "    snapshots[category][name] = z_vectors\n",
    "        \n",
    "    return z_vectors\n",
    "\n",
    "def get_snapshot(category, name):\n",
    "    return snapshots[category][name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_vectors = create_snapshot(category, 'snapshot1', z_vectors)\n",
    "display_gallery(generator, z_vectors, class_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_vectors = randomize()\n",
    "display_gallery(generator, z_vectors, class_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_vectors = get_snapshot(category, 'snapshot1')\n",
    "display_gallery(generator, z_vectors, class_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting Images\n",
    "One of the goals of interactive GAN model exploration is to select latent vectors $\\mathbf{z}$ that result in photo-realistic images to generate representative galleries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_sample(category, z_vectors, sample_idx, rows=5, cols=5):\n",
    "    if category not in select_samples:\n",
    "        select_samples[category] = []\n",
    "        \n",
    "    select_samples[category].append(z_vectors[get_index_of(sample_idx[0], sample_idx[1], rows, cols)])\n",
    "        \n",
    "    return z_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sample_gallery():\n",
    "    \n",
    "    sample_rows = 0\n",
    "    sample_cols = 0\n",
    "    sample_z_vectors = []\n",
    "    sample_class_vector = []\n",
    "    \n",
    "    for category in select_samples:\n",
    "        for z_vector in select_samples[category]:\n",
    "            sample_z_vectors.append(z_vector)\n",
    "            sample_class_vector.append(category)\n",
    "    \n",
    "    # Calculate gallery size. This is a very naive implementation.\n",
    "    for i in range(5,0,-1):\n",
    "        if len(sample_z_vectors) % i == 0:\n",
    "            sample_cols = i\n",
    "            sample_rows = len(sample_z_vectors) // sample_cols\n",
    "            break\n",
    "    \n",
    "    return np.asarray(sample_z_vectors), one_hot_from_names(sample_class_vector, batch_size=len(sample_class_vector)), sample_rows, sample_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_vectors = select_sample(category, z_vectors, (4, 4))\n",
    "display_gallery(generator, z_vectors, class_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_z_vectors, sample_class_vector, sample_rows, sample_cols = create_sample_gallery()\n",
    "display_gallery(generator, sample_z_vectors, sample_class_vector, sample_rows, sample_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the next cell to explore the model and locate more photo-realistic images, then use the create_sample_gallery() and display_gallery() functions to display samples that you have collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add one or more cells in which you will use the interactions we covered so far to explore the model and select photo-realistic images. Use the select_sample() function to select images. Then run the cell below to show your gallery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_z_vectors, sample_class_vector, sample_rows, sample_cols = create_sample_gallery()\n",
    "display_gallery(generator, sample_z_vectors, sample_class_vector, sample_rows, sample_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zoom into Region\n",
    "At times, the user might encounter a working image gallery that contains a set of promissing images in a certain sub-region of the gallery. The user may want to then explore that particular region. Let's see how we can implement an interaction that supports this.\n",
    "\n",
    "Mathematically, let the current working gallery be defined by a plane region $\\mathcal{P}$, and let the vectors of latent variables corresponding to the top-left and bottom-right images in the zoom in region be $\\mathbf{z_i}$ and $\\mathbf{z_j}$, respectively. To construct the resulting working gallery plane region $\\mathcal{P'}$ by computing vectors $\\mathbf{c'},\\mathbf{u'},\\mathbf{v'}$ using the following equations:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathbf{c'}=\\frac{\\mathbf{z_i}+\\mathbf{z_j}}{2}\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "    \\mathbf{u'}=\\frac{\\mathbf{z_i}-\\mathbf{z_j}}{2}\n",
    "\\end{equation}\n",
    "\\begin{equation}\\label{v-before-norm}\n",
    "    \\mathbf{v'}=\\frac{\\left\\|\\mathbf{u'}\\right\\|}{\\left\\|\\mathbf{v_1}-\\mathbf{v_2}\\right\\|} (\\mathbf{v_1}-\\mathbf{v_2})\n",
    "\\end{equation}\n",
    "where \n",
    "\\begin{gather*}\n",
    "    \\mathbf{v_1}=\\mathbf{z_i}-2\\mathbf{z_j} \\\\\n",
    "    \\mathbf{v_2}=\\frac{\\mathbf{v_1}\\cdot \\mathbf{u'}}{\\mathbf{u'}\\cdot \\mathbf{u'}}\\mathbf{u'}\n",
    "\\end{gather*}\n",
    "\n",
    "In the equation above, $\\mathbf{v_1}-\\mathbf{v_2}$ is the vector rejection of $\\mathbf{v_1}$ from $\\mathbf{u'}$, where $\\mathbf{v_1}-\\mathbf{v_2}$ ensures that $\\mathbf{v'}$ is perpendicular to $\\mathbf{u'}$, while the $\\frac{\\left\\|\\mathbf{u'}\\right\\|}{\\left\\|\\mathbf{v_1}-\\mathbf{v_2}\\right\\|}$ term scales the vector so that $\\left\\|\\mathbf{u'}\\right\\|=\\left\\|\\mathbf{v'}\\right\\|$. Our choice of $\\mathbf{v_1}$ ensures that $\\mathbf{v_1} \\nparallel \\mathbf{u'}$ (otherwise, $\\mathbf{v'}$ would become zero), while $\\mathbf{v_2}$ computes the vector projection of $\\mathbf{v_1}$ from $\\mathbf{u'}$. Note that this operation does not change the \"orientation\" of the resulting plane region.\n",
    "\n",
    "In this implementation, the resulting plane region is always square, even if the zoom-in-region in the interface is rectangular:\n",
    "\n",
    "<div><img src='img/zoom-into-region-diagram.png' width='40%'/></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zoom_into_region(z_vectors, start_idx, end_idx, truncation=0.04, num_dims = 10, rows=5, cols=5):\n",
    "\n",
    "    sampling_points = compute_sampling_points(z_vectors, tune_idxs)\n",
    "    \n",
    "    point1 = sampling_points[get_index_of(start_idx[0], start_idx[1], rows, cols)]\n",
    "    point2 = sampling_points[get_index_of(end_idx[0], end_idx[1], rows, cols)]\n",
    "    \n",
    "    c = (point1 + point2) / 2\n",
    "    u = point1 - point2\n",
    "    u /= np.linalg.norm(u)\n",
    "\n",
    "    v = point1 - 2 * point2 \n",
    "    v -= v.dot(u) * u\n",
    "    v /= np.linalg.norm(v)\n",
    "\n",
    "    zoom_level = math.log(30 / np.linalg.norm(point2 - point1), 1.2)\n",
    "\n",
    "    sampling_points = sampling_from_plane(c, u, v) \n",
    "\n",
    "    z_vectors = compute_z_vectors(sampling_points, base_vector, tune_idxs)\n",
    "    \n",
    "    return z_vectors, zoom_level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's refresh our memory about our current working gallery, so that we do not have to scroll up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_gallery(generator, z_vectors, class_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now select a region you want to zoom into. What do you notice about the resulting image gallery?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_vectors, zoom_level = zoom_into_region(z_vectors, (1,4), (4,5))\n",
    "display_gallery(generator, z_vectors, class_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The images in the top-left and bottom-right corners of the resulting gallery are the images that bounded your region."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Turn!\n",
    "Now it is time for you to think of an interaction that you think would help you explore this model. Think about the image gallery paradigm and how you can manipulate the lens onto the model. Feel free to modify the rest of this notebook and implement as many functions that you need. Test and showecase your function, then select some images. We will have an opportunity to show your work at the end of the session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: your code starts here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or alternatively, you can attempt to implement zoom in and out functionality, using Bayesian optimization. Note that this might be more difficult given technical constraints than it looks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zoom In and Out\n",
    "To explore the search space around a particular image of interest in the current working gallery, the user should be able to zoom in or zoom out of the current gallery centered at an image. The resulting working gallery will have the image that the user zoomed in or out of in the center and it will be surrounded by images that are more similar to it than the images in the current working gallery. Note, however, that zoom out is not a simple \"undo\" of the zoom in (and vice versa) because the user could use other tools after zooming into an image, or zoom out of an image that the user has not previously zoomed into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pySequentialLineSearch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# You may have to compile this python library for this import to work.\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpySequentialLineSearch\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mzoom\u001b[39m(z_vectors, zoom_idx, direction, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.04\u001b[39m, num_dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m, rows\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, cols\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m      5\u001b[0m     optimizer \u001b[38;5;241m=\u001b[39m pySequentialLineSearch\u001b[38;5;241m.\u001b[39mSequentialLineSearchOptimizer(num_dims\u001b[38;5;241m=\u001b[39mnum_dims)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pySequentialLineSearch'"
     ]
    }
   ],
   "source": [
    "# You may have to compile this python library for this import to work.\n",
    "import pySequentialLineSearch\n",
    "\n",
    "def zoom(z_vectors, zoom_idx, direction, truncation=0.04, num_dims = 10, rows=5, cols=5):\n",
    "    optimizer = pySequentialLineSearch.SequentialLineSearchOptimizer(num_dims=num_dims)\n",
    "    optimizer.set_hyperparams(kernel_signal_var=0.50,\n",
    "                        kernel_length_scale=0.10,\n",
    "                        kernel_hyperparams_prior_var=0.10) \n",
    "\n",
    "    new_zoom_level = zoom_level + 1 if direction == \"in\" else zoom_level - 1\n",
    "    \n",
    "    sampling_points = compute_sampling_points(z_vectors, tune_idxs)\n",
    "\n",
    "    c = sampling_points[pick_idx].reshape(-1, 1)\n",
    "    \n",
    "    # TODO:\n",
    "\n",
    "    sampling_points = sampling_from_plane(c, u, v) \n",
    "\n",
    "    z_vectors = compute_z_vectors(sampling_points, base_vector, tune_idxs)\n",
    "    \n",
    "    return z_vectors, zoom_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'zoom' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m z_vectors, zoom_level \u001b[38;5;241m=\u001b[39m \u001b[43mzoom\u001b[49m(z_vectors, (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124min\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m display_gallery(generator, z_vectors, class_vector)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'zoom' is not defined"
     ]
    }
   ],
   "source": [
    "z_vectors, zoom_level = zoom(z_vectors, (3, 3), 'in')\n",
    "display_gallery(generator, z_vectors, class_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "Enhao Zhang and Nikola Banovic. 2021. Method for Exploring Generative Adversarial Networks (GANs) via Automatically Generated Image Galleries. In <i>Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems</i> (<i>CHI '21</i>). Association for Computing Machinery, New York, NY, USA, Article 76, 1â€“15. https://doi.org/10.1145/3411764.3445714\n",
    "\n",
    "Yuki Koyama, Issei Sato, and Masataka Goto. 2020. Sequential gallery for interactive visual design optimization. <i>ACM Trans. Graph.</i> 39, 4, Article 88 (August 2020), 12 pages. https://doi.org/10.1145/3386569.3392444"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
